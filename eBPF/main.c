// Minimal user-space loader/reader using libbpf skeleton.
#define _GNU_SOURCE
#include "sched_monitor.h"
#include <errno.h>
#include <dirent.h>
#include <signal.h>
#include <stdio.h>
#include <stdint.h>
#include <stdbool.h>
#include <time.h>
#include <unistd.h>
#include <sys/resource.h>
#include <sys/stat.h>
#include <limits.h>
#include <bpf/libbpf.h>
#include <bpf/bpf.h>

#include "sched_monitor.skel.h" // generated by bpftool

static volatile sig_atomic_t exiting = 0;
static void on_sigint(int signo) { (void)signo; exiting = 1; }

static const uint64_t runtime_threshold_ns = DEFAULT_RUNTIME_THRESHOLD_NS;
static const char target_cgroup_root[] = "/sys/fs/cgroup/gocker";


static const char *evt_name(uint32_t t)
{
    switch (t) {
    case 1: return "SWITCH";
    case 2: return "WAKEUP";
    case 3: return "RUNTIME";
    case 4: return "MIGRATE";
    default: return "?";
    }
}

static int handle_event(void *ctx, void *data, size_t len)
{
    const struct evt *e = data;
    (void)ctx; (void)len;


    printf("ts=%llu cpu=%u type=%s pid=%u tgid=%u cgid=%llu aux0=%d aux1=%d aux2=%llu comm=%s\n",
            (unsigned long long)e->ts,
            e->cpu,
            evt_name(e->type),
            e->pid,
            e->tgid,
            (unsigned long long)e->cgroup_id,
            e->aux0,
            e->aux1,
            (unsigned long long)e->aux2,
            e->comm);
    return 0;
}


static int bump_memlock_rlimit(void)
{
    struct rlimit rl = {RLIM_INFINITY, RLIM_INFINITY};
    return setrlimit(RLIMIT_MEMLOCK, &rl);
}


static void dump_cgstats(struct sched_monitor_bpf *skel)
{
    int map_fd = bpf_map__fd(skel->maps.cg_stats);
    __u64 key = 0, next_key = 0;
    struct cg_stat_val val;


    printf("=== per-cgroup stats (runtime >= %lluns) ===\n",
            (unsigned long long)runtime_threshold_ns);
    int ret = bpf_map_get_next_key(map_fd, NULL, &key);
    while (ret == 0) {
        if (bpf_map_lookup_elem(map_fd, &key, &val) == 0) {
            if (runtime_threshold_ns && val.runtime_ns < runtime_threshold_ns) {
                ret = bpf_map_get_next_key(map_fd, &key, &next_key);
                key = next_key;
                continue;
            }
            printf("cgid=%llu runtime_ns=%llu max_runtime_ns=%llu ctx_switches=%llu wakeups=%llu migrations=%llu\n",
                    (unsigned long long)key,
                    (unsigned long long)val.runtime_ns,
                    (unsigned long long)val.max_runtime_ns,
                    (unsigned long long)val.ctx_switches,
                    (unsigned long long)val.wakeups,
                    (unsigned long long)val.migrations);
        }
        ret = bpf_map_get_next_key(map_fd, &key, &next_key);
        key = next_key;
    }
}

static int add_allowed_cgroup(int map_fd, __u64 cgid)
{
    uint8_t one = 1;
    if (bpf_map_update_elem(map_fd, &cgid, &one, BPF_ANY) < 0)
        return -errno;
    return 0;
}

static int walk_cgroup_tree(int map_fd, const char *path)
{
    struct stat st;

    if (stat(path, &st) < 0)
        return -errno;
    if (!S_ISDIR(st.st_mode))
        return -ENOTDIR;

    int err = add_allowed_cgroup(map_fd, st.st_ino);
    if (err)
        return err;

    DIR *dir = opendir(path);
    if (!dir)
        return -errno;

    struct dirent *entry;
    while (1) {
        errno = 0;
        entry = readdir(dir);
        if (!entry) {
            if (err == 0 && errno != 0)
                err = -errno;
            break;
        }

        if (entry->d_name[0] == '.' &&
            (entry->d_name[1] == '\0' ||
             (entry->d_name[1] == '.' && entry->d_name[2] == '\0')))
            continue;

        if (entry->d_type != DT_DIR && entry->d_type != DT_UNKNOWN)
            continue;

        char child_path[PATH_MAX];
        int len = snprintf(child_path, sizeof(child_path), "%s/%s", path, entry->d_name);
        if (len < 0 || len >= (int)sizeof(child_path)) {
            err = -ENAMETOOLONG;
            break;
        }

        struct stat child_stat;
        if (stat(child_path, &child_stat) < 0) {
            err = -errno;
            break;
        }
        if (!S_ISDIR(child_stat.st_mode))
            continue;

        err = walk_cgroup_tree(map_fd, child_path);
        if (err)
            break;
    }
    closedir(dir);
    return err;
}

static int populate_allowed_cgroups(struct sched_monitor_bpf *skel, const char *root_path)
{
    int map_fd = bpf_map__fd(skel->maps.allowed_cgroups);
    if (map_fd < 0)
        return -errno;

    return walk_cgroup_tree(map_fd, root_path);
}

int main(int argc, char **argv)
{
    (void)argc; (void)argv;
    struct ring_buffer *rb = NULL;
    struct sched_monitor_bpf *skel = NULL;
    int err;


    libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
    libbpf_set_print((libbpf_print_fn_t)NULL); // silence; comment if debugging


    if ((err = bump_memlock_rlimit())) {
        fprintf(stderr, "setrlimit(RLIMIT_MEMLOCK) failed: %d\n", err);
        return 1;
    }


    skel = sched_monitor_bpf__open();  // open .o檔
    if (!skel) {
        fprintf(stderr, "failed to open BPF skeleton\n");
        return 1;
    }


    if (skel->rodata) {
        skel->rodata->runtime_event_threshold_ns = runtime_threshold_ns;
        skel->rodata->filter_enabled = 1;
    }


    if ((err = sched_monitor_bpf__load(skel))) {  // 載入 BPF program 
        fprintf(stderr, "failed to load and verify BPF programs: %d\n", err);
        goto cleanup;
    }


    if ((err = populate_allowed_cgroups(skel, target_cgroup_root))) {
        fprintf(stderr, "failed to populate cgroup filter from %s: %d\n",
                target_cgroup_root, err);
        goto cleanup;
    }


    if ((err = sched_monitor_bpf__attach(skel))) {   // 掛載 tracepoint 
        fprintf(stderr, "failed to attach BPF programs: %d\n", err);
        goto cleanup;
    }


    rb = ring_buffer__new(bpf_map__fd(skel->maps.events_rb), handle_event, NULL, NULL);
    if (!rb) {
        fprintf(stderr, "failed to create ring buffer\n");
        goto cleanup;
    }


    signal(SIGINT, on_sigint);
    signal(SIGTERM, on_sigint);


    printf("running... Ctrl-C to exit.\n");


    // simple poll loop with periodic stats dump
    uint64_t last_dump_ms = 0;
    while (!exiting) {
        int ret = ring_buffer__poll(rb, 200 /* ms */);   // read event
        if (ret < 0 && ret != -EINTR) {
            fprintf(stderr, "ring_buffer__poll: %d\n", ret);
        break;
        }
        struct timespec ts;
        clock_gettime(CLOCK_MONOTONIC, &ts);
        uint64_t now_ms = (uint64_t)ts.tv_sec * 1000 + ts.tv_nsec/1000000;
        if (now_ms - last_dump_ms >= 2000) {
            dump_cgstats(skel);
            last_dump_ms = now_ms;
        }
    }


cleanup:
    ring_buffer__free(rb);
    sched_monitor_bpf__destroy(skel);  // 清理
    return err != 0;
}
