// Minimal user-space loader/reader using libbpf skeleton.
#define _GNU_SOURCE
#include "sched_monitor.h"
#include <errno.h>
#include <signal.h>
#include <stdio.h>
#include <stdint.h>
#include <stdbool.h>
#include <time.h>
#include <unistd.h>
#include <sys/resource.h>
#include <sys/stat.h>
#include <string.h>
#include <bpf/libbpf.h>
#include <bpf/bpf.h>

#include "sched_monitor.skel.h" // generated by bpftool

static volatile sig_atomic_t exiting = 0;
static void on_sigint(int signo) { (void)signo; exiting = 1; }

struct filter_ctx {
    __u64 target_cgid;
};


static const char *evt_name(uint32_t t)
{
    switch (t) {
    case 1: return "SWITCH";
    case 2: return "WAKEUP";
    case 3: return "RUNTIME";
    case 4: return "MIGRATE";
    default: return "?";
    }
}

static int handle_event(void *ctx, void *data, size_t len)
{
    const struct evt *e = data;
    const struct filter_ctx *fctx = ctx;
    (void)len;

    if (fctx && fctx->target_cgid && e->cgroup_id != fctx->target_cgid)
        return 0;


    printf("ts=%llu cpu=%u type=%s pid=%u tgid=%u cgid=%llu aux0=%d aux1=%d aux2=%llu comm=%s\n",
            (unsigned long long)e->ts,
            e->cpu,
            evt_name(e->type),
            e->pid,
            e->tgid,
            (unsigned long long)e->cgroup_id,
            e->aux0,
            e->aux1,
            (unsigned long long)e->aux2,
            e->comm);
    return 0;
}


static int bump_memlock_rlimit(void)
{
    struct rlimit rl = {RLIM_INFINITY, RLIM_INFINITY};
    return setrlimit(RLIMIT_MEMLOCK, &rl);
}


static void dump_cgstats(struct sched_monitor_bpf *skel, __u64 target_cgid)
{
    int map_fd = bpf_map__fd(skel->maps.cg_stats);
    __u64 key = 0, next_key = 0;
    struct cg_stat_val val;


    printf("=== per-cgroup stats ===\n");
    if (target_cgid) {
        if (bpf_map_lookup_elem(map_fd, &target_cgid, &val) == 0) {
            printf("cgid=%llu runtime_ns=%llu ctx_switches=%llu wakeups=%llu migrations=%llu\n",
                    (unsigned long long)target_cgid,
                    (unsigned long long)val.runtime_ns,
                    (unsigned long long)val.ctx_switches,
                    (unsigned long long)val.wakeups,
                    (unsigned long long)val.migrations);
        } else {
            printf("cgid=%llu not present\n", (unsigned long long)target_cgid);
        }
        return;
    }

    int ret = bpf_map_get_next_key(map_fd, NULL, &key);
    while (ret == 0) {
        if (bpf_map_lookup_elem(map_fd, &key, &val) == 0) {
            printf("cgid=%llu runtime_ns=%llu ctx_switches=%llu wakeups=%llu migrations=%llu\n",
                    (unsigned long long)key,
                    (unsigned long long)val.runtime_ns,
                    (unsigned long long)val.ctx_switches,
                    (unsigned long long)val.wakeups,
                    (unsigned long long)val.migrations);
        }
        ret = bpf_map_get_next_key(map_fd, &key, &next_key);
        key = next_key;
    }
}

static void dump_pidstats(struct sched_monitor_bpf *skel, __u64 target_cgid)
{
    int map_fd = bpf_map__fd(skel->maps.pid_stats);
    struct pid_stat_key key, next_key;
    struct pid_stat_val val;

    printf("=== per-pid stats ===\n");

    int ret = bpf_map_get_next_key(map_fd, NULL, &key);
    while (ret == 0) {
        if (key.cgid == target_cgid) {
            if (bpf_map_lookup_elem(map_fd, &key, &val) == 0) {
                printf("pid=%u runtime_ns=%llu ctx_switches=%llu wakeups=%llu migrations=%llu\n",
                        key.pid,
                        (unsigned long long)val.runtime_ns,
                        (unsigned long long)val.ctx_switches,
                        (unsigned long long)val.wakeups,
                        (unsigned long long)val.migrations);
            }
        }
        ret = bpf_map_get_next_key(map_fd, &key, &next_key);
        key = next_key;
    }
}

static int resolve_cgroup_id(const char *path, __u64 *cgid)
{
    struct stat st;

    if (stat(path, &st) != 0) {
        fprintf(stderr, "stat(%s) failed: %s\n", path, strerror(errno));
        return -1;
    }

    *cgid = st.st_ino;
    return 0;
}

int main(int argc, char **argv)
{
    (void)argc; (void)argv;
    struct ring_buffer *rb = NULL;
    struct sched_monitor_bpf *skel = NULL;
    int err;
    const char *target_cg_path = "/sys/fs/cgroup/user.slice/user-1000.slice";
    __u64 target_cgid = 0;
    struct filter_ctx fctx = {0};


    libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
    libbpf_set_print((libbpf_print_fn_t)NULL); // silence; comment if debugging


    if (resolve_cgroup_id(target_cg_path, &target_cgid))
        return 1;

    if ((err = bump_memlock_rlimit())) {
        fprintf(stderr, "setrlimit(RLIMIT_MEMLOCK) failed: %d\n", err);
        return 1;
    }


    skel = sched_monitor_bpf__open();
    if (!skel) {
        fprintf(stderr, "failed to open BPF skeleton\n");
        return 1;
    }


    skel->rodata->target_cgid = target_cgid;


    if ((err = sched_monitor_bpf__load(skel))) {
        fprintf(stderr, "failed to load and verify BPF programs: %d\n", err);
        goto cleanup;
    }


    if ((err = sched_monitor_bpf__attach(skel))) {
        fprintf(stderr, "failed to attach BPF programs: %d\n", err);
        goto cleanup;
    }


    fctx.target_cgid = target_cgid;
    rb = ring_buffer__new(bpf_map__fd(skel->maps.events_rb), handle_event, &fctx, NULL);
    if (!rb) {
        fprintf(stderr, "failed to create ring buffer\n");
        goto cleanup;
    }


    signal(SIGINT, on_sigint);
    signal(SIGTERM, on_sigint);


    printf("filtering to cgroup %s (cgid=%llu)\n",
            target_cg_path, (unsigned long long)target_cgid);
    printf("running... Ctrl-C to exit.\n");


    // simple poll loop with periodic stats dump
    uint64_t last_dump_ms = 0;
    while (!exiting) {
        int ret = ring_buffer__poll(rb, 200 /* ms */);
        if (ret < 0 && ret != -EINTR) {
            fprintf(stderr, "ring_buffer__poll: %d\n", ret);
        break;
        }
        struct timespec ts;
        clock_gettime(CLOCK_MONOTONIC, &ts);
        uint64_t now_ms = (uint64_t)ts.tv_sec * 1000 + ts.tv_nsec/1000000;
        if (now_ms - last_dump_ms >= 2000) {
            dump_cgstats(skel, target_cgid);
            dump_pidstats(skel, target_cgid);
            last_dump_ms = now_ms;
        }
    }


cleanup:
    ring_buffer__free(rb);
    sched_monitor_bpf__destroy(skel);
    return err != 0;
}
